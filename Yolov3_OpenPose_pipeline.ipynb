{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxIiGFEyH1Ug"
      },
      "source": [
        "#Yolov3 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGqtFTWTICY8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(\"YOLOV3_Files\"):\n",
        "    os.makedirs(\"YOLOV3_Files\")\n",
        "\n",
        "# Download YOLOv3 files into the YOLOV3_Files folder\n",
        "if not os.path.exists(\"YOLOV3_Files/yolov3.weights\"):\n",
        "    os.system(\"wget -P YOLOV3_Files https://pjreddie.com/media/files/yolov3.weights\")\n",
        "if not os.path.exists(\"YOLOV3_Files/yolov3.cfg\"):\n",
        "    os.system(\"wget -P YOLOV3_Files https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\")\n",
        "if not os.path.exists(\"YOLOV3_Files/coco.names\"):\n",
        "    os.system(\"wget -P YOLOV3_Files https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\")\n",
        "\n",
        "print(\"Files have been downloaded to YOLOV3_Files folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtjmmH86Khrt"
      },
      "outputs": [],
      "source": [
        "cfg_path = \"/content/YOLOV3_Files/yolov3.cfg\"\n",
        "weights_path = \"/content/YOLOV3_Files/yolov3.weights\"\n",
        "names_path = \"/content/YOLOV3_Files/coco.names\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa09sjbOJg22"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# Load YOLOv3 model (same as before)\n",
        "\n",
        "def load_yolov3_model(cfg_path, weights_path, names_path):\n",
        "    net = cv2.dnn.readNet(weights_path, cfg_path)\n",
        "    # Use GPU if available\n",
        "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)  # Enable CUDA backend\n",
        "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "    #net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "    #net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
        "\n",
        "    # Load class names\n",
        "    with open(names_path, \"r\") as f:\n",
        "        classes = f.read().strip().split(\"\\n\")\n",
        "\n",
        "    return net, classes\n",
        "\n",
        "# Detect pedestrians in a single frame\n",
        "def detect_pedestrians_in_frame(frame, model, classes, confidence_threshold=0.2, nms_threshold=0.2):\n",
        "\n",
        "    # Get frame dimensions\n",
        "    height, width = frame.shape[:2]\n",
        "\n",
        "    # Prepare the frame for YOLO (creating a blob)\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
        "    model.setInput(blob)\n",
        "\n",
        "    # Get output layer names and perform a forward pass\n",
        "    layer_names = model.getUnconnectedOutLayersNames()\n",
        "    detections = model.forward(layer_names)\n",
        "\n",
        "    boxes, confidences, class_ids = [], [], []\n",
        "    pedestrian_class_id = classes.index(\"person\")\n",
        "\n",
        "    for output in detections:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "\n",
        "            if class_id == pedestrian_class_id and confidence > confidence_threshold:\n",
        "                # Scale bounding box to image size\n",
        "                box = detection[0:4] * np.array([width, height, width, height])\n",
        "                (center_x, center_y, box_width, box_height) = box.astype(\"int\")\n",
        "                x = int(center_x - (box_width / 2))\n",
        "                y = int(center_y - (box_height / 2))\n",
        "                boxes.append([x, y, int(box_width), int(box_height)])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "              #  print(\"Box:\", box)\n",
        "              #  print(\"Confidence:\", confidence)\n",
        "              #  print(\"Class ID:\", class_id)\n",
        "\n",
        "    # Apply non-maxima suppression to remove overlapping boxes\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
        "    # print(\"Indices before flattening:\", indices)\n",
        "\n",
        "    # Filter boxes based on indices\n",
        "    if len(indices) > 0:\n",
        "        indices = indices.flatten()\n",
        "        filtered_boxes = [boxes[i] for i in indices]\n",
        "    else:\n",
        "        filtered_boxes = []\n",
        "\n",
        "    #print(\"Filtered Boxes:\", filtered_boxes)\n",
        "    #print(\"Indices after flattening:\", indices)\n",
        "    print(\"Frame Finished\")\n",
        "    return filtered_boxes, indices\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgEvAkwnUJPq"
      },
      "source": [
        "# Object Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ULCXEPs4YWe-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Intersection over Union (IoU) calculation\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    unionArea = boxAArea + boxBArea - interArea\n",
        "    iou =  interArea / unionArea if unionArea != 0 else 0\n",
        "\n",
        "    return iou\n",
        "\n",
        "# Track objects across frames using IoU matching\n",
        "def track_objects(prev_boxes, curr_boxes, iou_threshold=0.1):\n",
        "    matches = []\n",
        "    unmatched_prev = list(range(len(prev_boxes)))\n",
        "    unmatched_curr = list(range(len(curr_boxes)))\n",
        "\n",
        "    for i, prev_box in enumerate(prev_boxes):\n",
        "        best_match = None\n",
        "        max_iou = 0\n",
        "\n",
        "        for j, curr_box in enumerate(curr_boxes):\n",
        "            if j in unmatched_curr:  # Ensure that curr_box is not matched yet\n",
        "                iou_value = iou(prev_box, curr_box)\n",
        "                if iou_value > max_iou and iou_value > iou_threshold:\n",
        "                    best_match = j\n",
        "                    max_iou = iou_value\n",
        "\n",
        "        if best_match is not None:\n",
        "            matches.append((i, best_match))\n",
        "            unmatched_prev.remove(i)\n",
        "            unmatched_curr.remove(best_match)\n",
        "    return matches, unmatched_prev, unmatched_curr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install OpenPose"
      ],
      "metadata": {
        "id": "bbK9aMf0xTbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git'\n",
        "git_path = '/content/lightweight-human-pose-estimation.pytorch'\n",
        "pre_model_path = os.path.join(git_path,'pre_model')\n",
        "\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !wget \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\" -P $git_path\n",
        "  !wget \"https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\" -P $pre_model_path\n",
        "  !mkdir coco && unzip \"/content/lightweight-human-pose-estimation.pytorch/annotations_trainval2017.zip\" -d \"/content/coco/\"\n",
        "\n",
        "%cd $git_path"
      ],
      "metadata": {
        "id": "r1qgHs1FN7Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "zmRkfyoeOT5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/lightweight-human-pose-estimation.pytorch')\n",
        "\n",
        "import argparse\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "from val import normalize, pad_width\n",
        "\n",
        "\n",
        "class ImageReader(object):\n",
        "    def __init__(self, file_names):\n",
        "        self.file_names = file_names\n",
        "        self.max_idx = len(file_names)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.idx = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.idx == self.max_idx:\n",
        "            raise StopIteration\n",
        "        img = cv2.imread(self.file_names[self.idx], cv2.IMREAD_COLOR)\n",
        "        if img.size == 0:\n",
        "            raise IOError('Image {} cannot be read'.format(self.file_names[self.idx]))\n",
        "        self.idx = self.idx + 1\n",
        "        return img\n",
        "\n",
        "\n",
        "class VideoReader(object):\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        try:  # OpenCV needs int to read from webcam\n",
        "            self.file_name = int(file_name)\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.cap = cv2.VideoCapture(self.file_name)\n",
        "        if not self.cap.isOpened():\n",
        "            raise IOError('Video {} cannot be opened'.format(self.file_name))\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        was_read, img = self.cap.read()\n",
        "        if not was_read:\n",
        "            raise StopIteration\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n",
        "               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n",
        "    height, width, _ = img.shape\n",
        "    scale = net_input_height_size / height\n",
        "\n",
        "    scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "    scaled_img = normalize(scaled_img, img_mean, img_scale)\n",
        "    min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n",
        "    padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n",
        "\n",
        "    tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float().to('cuda')  # Move directly to CUDA\n",
        "    if not cpu:\n",
        "      tensor_img = tensor_img.cuda()\n",
        "      net.cuda()\n",
        "\n",
        "    # Replace '+' with a forward pass through the network\n",
        "    stages_output = net(tensor_img)\n",
        "\n",
        "    stage2_heatmaps = stages_output[-2]\n",
        "    heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    stage2_pafs = stages_output[-1]\n",
        "    pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return heatmaps, pafs, scale, pad"
      ],
      "metadata": {
        "id": "dzOyBMCgxaAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "EsDGIFImxeaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "keypoint_connections =[\n",
        "    [1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10], [1, 11],\n",
        "    [11, 12], [12, 13], [1, 0], [0, 14], [14, 16], [0, 15], [15, 17], [2, 16], [5, 17]\n",
        "]\n",
        "object_tracker = {}  # Global dictionary to store IDs and boxes\n",
        "next_id = 0  # Counter for assigning unique IDs\n",
        "\n",
        "def process_frame(frame, prev_detections, model, classes, net, height_size, stride, upsample_ratio, cpu, track, smooth, iou_threshold=0.1):\n",
        "    global object_tracker, next_id\n",
        "\n",
        "    # Detect pedestrians in the current frame (YOLO detection)\n",
        "    boxes, indices = detect_pedestrians_in_frame(frame, model, classes)\n",
        "\n",
        "    # Track objects using IoU\n",
        "    if prev_detections is not None:\n",
        "        matches, unmatched_prev, unmatched_curr = track_objects(prev_detections, boxes, iou_threshold)\n",
        "\n",
        "        # Update positions of matched IDs\n",
        "        for prev_idx, curr_idx in matches:\n",
        "            for obj_id, prev_box in object_tracker.items():\n",
        "                if prev_box == prev_detections[prev_idx]:  # Find corresponding ID\n",
        "                    object_tracker[obj_id] = boxes[curr_idx]\n",
        "                    break\n",
        "\n",
        "        # Assign new IDs to unmatched current boxes\n",
        "        for curr_idx in unmatched_curr:\n",
        "            object_tracker[next_id] = boxes[curr_idx]\n",
        "            next_id += 1\n",
        "\n",
        "        # Remove IDs for unmatched previous boxes\n",
        "        for prev_idx in unmatched_prev:\n",
        "            for obj_id, prev_box in list(object_tracker.items()):\n",
        "                if prev_box == prev_detections[prev_idx]:\n",
        "                    del object_tracker[obj_id]\n",
        "                    break\n",
        "    else:\n",
        "        # First frame: assign IDs to all detections\n",
        "        for box in boxes:\n",
        "            object_tracker[next_id] = box\n",
        "            next_id += 1\n",
        "\n",
        "    # Now process each tracked person and send ROI to OpenPose\n",
        "    current_poses = []  # To store poses for each person\n",
        "\n",
        "    for obj_id, (x, y, w, h) in object_tracker.items():\n",
        "        #print(f\"Object ID: {obj_id}, Bounding Box: x={x}, y={y}, w={w}, h={h}\")\n",
        "        # Crop the frame to get the ROI of the person\n",
        "        roi = frame[int(y):int(y + h), int(x):int(x + w)]\n",
        "\n",
        "    # Convert ROI to a PyTorch tensor (if necessary) and move to GPU\n",
        "        roi_tensor = torch.from_numpy(roi).float().to('cuda')\n",
        "\n",
        "    # Convert back to NumPy array before passing to OpenCV\n",
        "        roi = roi_tensor.cpu().numpy()\n",
        "\n",
        "    # Send the cropped ROI to OpenPose for keypoint detection\n",
        "        heatmaps, pafs, scale, pad = infer_fast(net, roi, height_size, stride, upsample_ratio, cpu)\n",
        "\n",
        "        # Post-process the keypoints (similar to your OpenPose post-processing)\n",
        "        all_keypoints_by_type = []\n",
        "        total_keypoints_num = 0\n",
        "        num_keypoints = 18\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, pose_entry_size=20, min_paf_score=0.05)\n",
        "\n",
        "        # Adjust the coordinates of the keypoints based on the original frame\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "           # print(f\"Keypoint {kpt_id}: x={all_keypoints[kpt_id, 0]:.2f}, y={all_keypoints[kpt_id, 1]:.2f}\")\n",
        "\n",
        "        # Store poses for later processing\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # Keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "            pose = Pose(pose_keypoints, pose_entries[n][18])\n",
        "            current_poses.append(pose)\n",
        "\n",
        "    # Draw bounding boxes and IDs\n",
        "    for obj_id, (x, y, w, h) in object_tracker.items():\n",
        "        color = (0, 255, 0)  # Green color for the bounding box\n",
        "        cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
        "        cv2.putText(frame, f\"ID {obj_id}\", (int(x), int(y - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    for i, pose in enumerate(current_poses):\n",
        "        pose.id = i\n",
        "    # Draw poses inside bounding boxes\n",
        "    for pose in current_poses:\n",
        "        # Find the corresponding bounding box for each pose by matching object_id\n",
        "        for obj_id, (x, y, w, h) in object_tracker.items():\n",
        "            # Ensure poses are drawn inside the bounding box\n",
        "            #if pose.id == obj_id:  # Match pose to the correct object ID\n",
        "                scaled_keypoints = pose.keypoints.copy()\n",
        "                if pose.id == obj_id:  # Only process the pose corresponding to the current object_id\n",
        "                    scaled_keypoints = pose.keypoints.copy()\n",
        "                # Scale and offset the keypoints to Assumingfit inside the bounding box\n",
        "                    for kpt in scaled_keypoints:\n",
        "                        if kpt[0] == -1 or kpt[1] == -1:\n",
        "                            continue  # Skip invalid keypoints\n",
        "                       # print(pose.id)\n",
        "                       # print(obj_id)\n",
        "    # Scale each keypoint to fit within the bounding box\n",
        "                        #print(f\"[Before] x={kpt[0]}, y={kpt[1]}\")\n",
        "                        kpt[0] = int(x + kpt[0] )  # Rescale X-coordinate and apply x offset (bounding box x)\n",
        "                        kpt[1] = int(y + kpt[1] )  # Rescale Y-coordinate and apply y offset (bounding box y)\n",
        "\n",
        "    # Clamp keypoints to ensure they don't go out of bounds\n",
        "                        #kpt[0] =  min(frame.shape[1] - 1, kpt[0])#kpt[0] = max(0, min(frame.shape[1] - 1, kpt[0]))\n",
        "                        #kpt[1] =  min(frame.shape[0] - 1, kpt[1])#kpt[1] = max(0, min(frame.shape[0] - 1, kpt[1]))\n",
        "                        #print(f\"[after] x={kpt[0]}, y={kpt[1]}\")\n",
        "                        #print(f\"[BB] Object ID: {obj_id}, Bounding Box: x={x}, y={y}, w={w}, h={h}\")\n",
        "                      # Red keypoints\n",
        "\n",
        "                        cv2.circle(frame, (kpt[0], kpt[1]), 5, (0, 0, 255), -1)\n",
        "                    for (start, end) in keypoint_connections:\n",
        "                        start_kpt = scaled_keypoints[start]\n",
        "                        end_kpt = scaled_keypoints[end]\n",
        "\n",
        "                # Ensure the keypoints exist\n",
        "                        if start_kpt[0] >= 0 and start_kpt[1] >= 0 and end_kpt[0] >= 0 and end_kpt[1] >= 0:\n",
        "                    # Draw a line between the keypoints\n",
        "                                cv2.line(frame, (start_kpt[0], start_kpt[1]), (end_kpt[0], end_kpt[1]), (0, 255, 0), 2)  # Green lines\n",
        "                # Draw the pose inside the bounding box\n",
        "                #pose.draw(frame)  #  pose.draw() correctly handles the drawing\n",
        "\n",
        "    # Return the frame with the keypoints drawn inside the bounding boxes\n",
        "    return frame, list(object_tracker.values())\n",
        "\n",
        "\n",
        "\n",
        "def display_video_with_tracking(video_path, output_path, model, classes, net, cpu=True, height_size=256, num_keypoints=18, track=True, smooth=True, iou_threshold=0.1):\n",
        "    cap = cv2.VideoCapture(video_path)  # Open the video file\n",
        "\n",
        "    # Get video properties\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Initialize the VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    prev_detections = None  # Initialize previous detections as None for the first frame\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Process the frame with detection, tracking, and pose estimation\n",
        "        frame, prev_detections = process_frame(\n",
        "            frame,\n",
        "            prev_detections,\n",
        "            model,\n",
        "            classes,\n",
        "            net,\n",
        "            height_size=height_size,\n",
        "            stride=8,  # Add stride value from OpenPose logic\n",
        "            upsample_ratio=4,  # Add upsample ratio value from OpenPose logic\n",
        "            cpu=cpu,\n",
        "            track=track,\n",
        "            smooth=smooth,\n",
        "            iou_threshold=iou_threshold\n",
        "        )\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release the video capture and writer objects\n",
        "    cap.release()\n",
        "    out.release()\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths and load the models\n",
        "    video_path = \"/content/Input.mp4\"\n",
        "    cfg_path = \"/content/YOLOV3_Files/yolov3.cfg\"\n",
        "    weights_path = \"/content/YOLOV3_Files/yolov3.weights\"\n",
        "    names_path = \"/content/YOLOV3_Files/coco.names\"\n",
        "    output_path = \"/content/MK_output.mp4\"\n",
        "    checkpoint_path = \"/content/lightweight-human-pose-estimation.pytorch/pre_model/checkpoint_iter_370000.pth\"\n",
        "\n",
        "    # Load YOLO model\n",
        "    model, classes = load_yolov3_model(cfg_path, weights_path, names_path)\n",
        "\n",
        "    # Load Pose Estimation model (OpenPose)\n",
        "    net = PoseEstimationWithMobileNet()\n",
        "    # After initializing PoseEstimationWithMobileNet\n",
        "    net = PoseEstimationWithMobileNet().to('cuda')\n",
        "    print(next(net.parameters()).device) # Move to GPU\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    load_state(net, checkpoint)\n",
        "\n",
        "    # Display the video with detection, tracking, and pose estimation\n",
        "    display_video_with_tracking(video_path, output_path, model, classes, net, cpu=False, height_size=256, num_keypoints=18, track=True, smooth=True, iou_threshold=0.1)\n"
      ],
      "metadata": {
        "id": "O3JxJJO4xnu2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}